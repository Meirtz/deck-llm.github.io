<!DOCTYPE html>
<html>
<head>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DeCK Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="hero-logo">
              <img src="static/images/logo.webp" alt="Logo">
            </div>
            <h1 class="title is-1 publication-title"><br>Decoding by Contrasting Knowledge: <br>Enhancing LLMsâ€™ Confidence <br> on Edited Facts</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a target="_blank">Baolong Bi</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                  <a href="https://shenghua-liu.github.io/" target="_blank">Shenghua Liu</a><sup>1,3*</sup>,
              </span>
              <span class="author-block">
                <a href="https://me.meirtz.com/" target="_blank">Lingrui Mei</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a herf="https://wangywust.github.io/" target="_blank">Yiwei Wang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Pengliang Ji</a><sup>4</sup>,
              </span>
              
              <span class="author-block">
                <a target="_blank">Xueqi Cheng</a><sup>1,3</sup>,
              </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>CAS Key Laboratory of AI Safty, Institute of Computing Technology, CAS<br><sup>2</sup>University of California, Los Angeles, <sup>3</sup>University of Chinese Academy of Sciences</span><br><sup>4</sup>Carnegie Mellon University</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author.</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2405.11613" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/byronBBL/DeCK" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2405.11613" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3"></h2>
      <div class="level-set has-text-justified">
     </div>
     <img src="static/images/showcase.png" class="blend-img-background center-image" style=" display: block; margin-left: auto; margin-right: auto; width: 100%; max-width: 800px; height: auto;">
</div>
   </div>
  </section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The knowledge within large language models (LLMs) may become outdated quickly.
            While in-context editing (ICE) is currently the most effective method for knowledge editing (KE), it is constrained by the black-box modeling of LLMs and thus lacks interpretability.
            Our work aims to elucidate the superior performance of ICE on the KE by analyzing the impacts of in-context new knowledge on token-wise distributions.
            We observe that despite a significant boost in logits of the new knowledge, the performance of is still hindered by stubborn knowledge. 
            Stubborn knowledge refers to as facts that have gained excessive confidence during pretraining, making it hard to edit effectively.
            To address this issue and further enhance the performance of ICE, we propose a novel approach termed $\textbf{De}$coding by $\textbf{C}$ontrasting $\textbf{K}$nowledge ($\textbf{DeCK}$).
            DeCK derives the distribution of the next token by contrasting the logits obtained from the newly edited knowledge guided by ICE with those from the unedited parametric knowledge. 
            Our experiments consistently demonstrate that DeCK enhances the confidence of LLMs in edited facts. 
            For instance, it improves the performance of LLaMA3-8B-instruct on MQuAKE by up to 219%, demonstrating its capability to strengthen ICE in the editing of stubborn knowledge.
            DeCK can be easily integrated into any ICE method as a decoding component to enhance editing capabilities.
            Our work paves the way to develop the both effective and accountable KE methods for LLMs.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<br>
<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Deep Insights into ICE through Decoding Perspectives</h2>
      <img src="static/images/observe.jpg" class="blend-img-background center-image" style=" display: block; margin-left: auto; margin-right: auto; width: 100%; max-width: 800px; height: auto;">
      <div class="level-set has-text-justified">
        <p>
          We explore why in-context editing (ICE) effectively edits knowledge in large language models (LLMs). Using dedicated experiments, we capture the logits output of knowledge influenced by the edit. Our results show that incorporating new knowledge through ICE significantly increases the predicted probability of generating edited facts during the decoding process.
        </p>
        <p>
          Formally, given a sequence of tokens \(X^E = \{x^E_1, x^E_2, ..., x^E_{m-1}\}\) which includes guidance of an editing prompt, the probability of the next token \(x^E_m\) with editing guidance is computed as:
        </p>
        <p>
          $$ P_E(x^E_m|x^E_{\lt m}) = \text{softmax}(\phi(h^E_m)) $$
        </p>
        <p>
          This contrasts with the parametric probability distribution \(P_B(x^B_n|x^B_{\lt n})\), which solely represents the response of LLMs based on their unedited knowledge. 
        </p>
        <p>
          Our analysis shows that the introduction of new knowledge through ICE shifts the probability distribution of new knowledge significantly, while the logits for parametric knowledge remain largely unchanged. This indicates that ICE enhances the logits of new knowledge, thereby improving LLMs' confidence in editing facts.
        </p>
      </div>
      <img src="static/images/figure3.jpg" class="blend-img-background center-image" style=" display: block; margin-left: auto; margin-right: auto; width: 100%; max-width: 800px; height: auto;">
  
      </div>
  </div>
</section>

<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">DeCK: Enhancing LLMsâ€™ Confidence on Edited Facts</h2>
      <div class="level-set has-text-justified">
        <p>
          Inspired by our observations, we propose Decoding by Contrasting Knowledge (DeCK) to enhance ICE's ability to overcome stubborn knowledge in LLMs. DeCK consists of two key components: an editing enhancement module and a contrastive decoding strategy.
        </p>
        <p>
          First, we improve attention to new knowledge by enhancing the probability distribution \(P_E\). We introduce the Knowledge Enhancement Divergence (KED) to measure the difference between the enhanced distribution and a target distribution \(Q\):
        </p>
        <p>
          $$ \text{KED}(P||Q) = \frac{1}{2} \sum_{i=1}^n w_i \left( P(v_i) \log \frac{P(v_i)}{M(v_i)} + Q(v_i) \log \frac{Q(v_i)}{M(v_i)} \right) $$
        </p>
        <p>
          Here, \(M = \frac{1}{2}(P + Q)\) is the average distribution, and \(w_i\) is a weight based on semantic relevance. The semantic relevance function \(s\) measures the relevance of a token \(v_i\) to the edited knowledge \(E\), defined as:
        </p>
        <p>
          $$ s(v_i, E) = \max_{e_j \in E} \text{sim}(v_i, e_j) \cdot \phi(v_i) $$
        </p>
        <p>
          where \(\text{sim}(\cdot, \cdot)\) is a similarity function, and \(\phi\) is a frequency-based weighting function. We enhance the logits \(\phi(h^E_m)\) using:
        </p>
        <p>
          $$ \text{Enh}(\phi(h^E_m), s) = \alpha \cdot \phi(h^E_m) + \beta \cdot s $$
        </p>
        <p>
          where \(\alpha\) and \(\beta\) are scaling coefficients.
        </p>
        <p>
          Next, we apply a contrastive decoding strategy by comparing the enhanced new knowledge distribution with the original parametric distribution to predict the next token. The probability of the next token is computed as:
        </p>
        <p>
          $$ \hat{P}_E(x^E_m) = \text{softmax} \left( F \left( P_E(x^E_m), P_B(x^B_n) \right) \right) $$
        </p>
        <p>
          where the function \(F\) is defined as:
        </p>
        <p>
          $$ F \left( P_E(x^E_m), P_B(x^B_n) \right) = \log \frac{P_E(x^E_m)}{P_B(x^B_n)} $$
        </p>
        <p>
          To ensure plausibility, we introduce an adaptive plausibility constraint (APC) strategy, filtering out tokens with low probabilities in \(P_E(x^E_m)\) and considering only high-score tokens. The subset \(V_{\text{head}}(x^E_m|x^E_{\lt m})\) is defined as:
        </p>
        <p>
          $$ V_{\text{head}}(x^E_m|x^E_{\lt m}) = \{ x^E_m \in V : P_E(x^E_m) \geq \lambda \max_w P_E(w) \} $$
        </p>
        <p>
          where \(\lambda\) is a threshold parameter.
        </p>
        <p>
          By contrasting these distributions, DeCK effectively amplifies the signal of new knowledge while suppressing the influence of parametric knowledge. Our experiments demonstrate that DeCK significantly boosts the confidence of LLMs in edited facts, enhancing the performance of ICE in editing stubborn knowledge.
        </p>
      </div>
      <img src="static/images/statics.jpg" class="blend-img-background center-image" style=" display: block; margin-left: auto; margin-right: auto; width: 100%; max-width: 800px; height: auto;">
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Experiments</h2>
      <img src="static/images/result0.jpg" class="blend-img-background center-image" style=" display: block; margin-left: auto; margin-right: auto; width: 100%; max-width: 800px; height: auto;">
    
      <div class="level-set has-text-justified">
        <p>
          We conducted extensive experiments using the MQUAKE-3K dataset and its derivatives to evaluate the effectiveness of DeCK. Our results show that DeCK significantly enhances the performance of ICE across various models and datasets. Specifically, DeCK has increased ICEâ€™s editing success rate in LLAMA2-13B-CHAT by 63% and in LLAMA3-8B-INSTRUCT by 219%.
        </p>
      </div>
      <img src="static/images/result1.jpg" class="blend-img-background center-image" style=" display: block; margin-left: auto; margin-right: auto; width: 100%; max-width: 800px; height: auto;">
      <img src="static/images/result2.jpg" class="blend-img-background center-image" style=" display: block; margin-left: auto; margin-right: auto; width: 100%; max-width: 800px; height: auto;">
      <p>For detailed results and further insights, please refer to our paper.</p>
    </div>
  </div>
</section>

  <section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Contributions</h2>
      <div class="level-set has-text-justified">
        <p>
          Our contributions can be summarized by three points. First, as far as we know, we are the first to elucidate superior performance of ICE on the KE from a model interpretability perspective. Second, we find that stubborn knowledge significantly impacts the performance of ICE, and we propose DeCK to boost confidence in editing facts, enhancing ICE to overcome it. Third, extensive experiments on \textsc{MQuAKE} indicate that our DeCK can effectively enhance the performance of ICE without altering the internal model or modifying external prompts. DeCK can be easily integrated into any ICE method as a decoding component to enhance editing capabilities. Our work paves the way to develop the both effective and accountable KE methods for LLMs.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Conclusion and Limitations</h2>
      <div class="level-set has-text-justified">
        <p>
          In this paper, we introduced Decoding by Contrasting Knowledge (DeCK), a novel decoding strategy aimed at enhancing in-context editing to overcome stubborn knowledge for LLMs. DeCK significantly improves editing accuracy but requires the reception of input from two different token sequences during the generation process, resulting in increased latency. Future work may focus on optimizing the transformers architecture or exploring alternative, more cost-effective versions of DeCK.
        </p>
      </div>
    </div>
  </div>
</section>












<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
           <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
